#!/usr/bin/env python3
"""
Validateur de S√©curit√© - Syst√®me d'Audit AGI
Responsabilit√© unique : D√©tection des probl√®mes de s√©curit√© dans le code Python
Respecte strictement la directive des 200 lignes
"""

import re
import ast
from pathlib import Path
from typing import Dict, List, NamedTuple, Set
from dataclasses import dataclass


class SecurityIssue(NamedTuple):
    """Repr√©sente un probl√®me de s√©curit√© d√©tect√©"""

    file_path: str
    issue_type: str
    description: str
    severity: str
    line_number: int
    pattern: str


@dataclass
class SecurityScanResult:
    """R√©sultat du scan de s√©curit√©"""

    scanned_files: int
    security_issues: List[SecurityIssue]
    high_risk_files: List[str]
    security_score: float


class SecurityValidator:
    """Validateur de s√©curit√© pour code Python"""

    def __init__(self):
        self.dangerous_patterns = {
            # Ex√©cution de code dangereux
            "code_execution": [
                (
                    r"eval\s*\(",
                    "Usage d'eval() - ex√©cution de code arbitraire",
                    "CRITICAL",
                ),
                (
                    r"exec\s*\(",
                    "Usage d'exec() - ex√©cution de code arbitraire",
                    "CRITICAL",
                ),
                (r"compile\s*\(", "Compilation dynamique de code", "HIGH"),
                (r"__import__\s*\(", "Import dynamique non s√©curis√©", "MEDIUM"),
            ],
            # Manipulation de fichiers dangereuse
            "file_operations": [
                (
                    r"open\s*\([^)]*['\"]w",
                    "√âcriture de fichier sans validation",
                    "MEDIUM",
                ),
                (r"os\.system\s*\(", "Ex√©cution de commandes syst√®me", "CRITICAL"),
                (r"subprocess\.call", "Appel subprocess sans validation", "HIGH"),
                (r"os\.popen\s*\(", "Ouverture de processus syst√®me", "HIGH"),
            ],
            # Gestion des chemins non s√©curis√©e
            "path_traversal": [
                (r"\.\.\/", "Travers√©e de r√©pertoire potentielle", "MEDIUM"),
                (r"os\.path\.join.*\.\.", "Path traversal dans os.path.join", "MEDIUM"),
                (
                    r"open\s*\([^)]*\/\.\.",
                    "Ouverture de fichier avec travers√©e",
                    "HIGH",
                ),
            ],
            # Injection potentielle
            "injection_risks": [
                (
                    r"\.format\s*\(.*input",
                    "Format string avec input utilisateur",
                    "MEDIUM",
                ),
                (r"% .*input", "String formatting avec input", "MEDIUM"),
                (r"sql.*\+.*input", "Concat√©nation SQL potentielle", "HIGH"),
            ],
        }

        self.secure_patterns = {
            "pathlib_usage": r"from pathlib import|import pathlib",
            "input_validation": r"validate|sanitize|clean",
            "secure_random": r"secrets\.|os\.urandom",
            "logging_security": r"logging\.|logger\.",
        }

    def scan_file(self, file_path: Path) -> List[SecurityIssue]:
        """Scanne un fichier pour d√©tecter les probl√®mes de s√©curit√©"""
        issues = []

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                lines = content.split("\n")

            # Scan par patterns de regex
            issues.extend(self._scan_dangerous_patterns(file_path, content, lines))

            # Analyse AST si possible
            try:
                tree = ast.parse(content)
                issues.extend(self._analyze_ast_security(file_path, tree))
            except SyntaxError:
                # Ignore les erreurs de syntaxe, g√©r√©es par le validateur syntaxique
                pass

        except Exception:
            # Probl√®me de lecture du fichier
            issues.append(
                SecurityIssue(
                    file_path=str(file_path),
                    issue_type="file_access_error",
                    description="Impossible de lire le fichier pour scan s√©curit√©",
                    severity="LOW",
                    line_number=0,
                    pattern="file_error",
                )
            )

        return issues

    def scan_directory(self, target_dir: Path) -> SecurityScanResult:
        """Scanne un r√©pertoire complet"""
        all_issues = []
        scanned_files = 0
        high_risk_files = []

        # Recherche r√©cursive des fichiers Python
        python_files = list(target_dir.rglob("*.py"))

        for py_file in python_files:
            file_issues = self.scan_file(py_file)
            all_issues.extend(file_issues)
            scanned_files += 1

            # Identification des fichiers √† haut risque
            critical_issues = [i for i in file_issues if i.severity == "CRITICAL"]
            high_issues = [i for i in file_issues if i.severity == "HIGH"]

            if critical_issues or len(high_issues) >= 2:
                high_risk_files.append(str(py_file))

        # Calcul du score de s√©curit√©
        security_score = self._calculate_security_score(all_issues, scanned_files)

        return SecurityScanResult(
            scanned_files=scanned_files,
            security_issues=all_issues,
            high_risk_files=high_risk_files,
            security_score=security_score,
        )

    def _scan_dangerous_patterns(
        self, file_path: Path, content: str, lines: List[str]
    ) -> List[SecurityIssue]:
        """Scanne les patterns dangereux via regex"""
        issues = []

        for category, patterns in self.dangerous_patterns.items():
            for pattern, description, severity in patterns:
                for line_num, line in enumerate(lines, 1):
                    if re.search(pattern, line):
                        issues.append(
                            SecurityIssue(
                                file_path=str(file_path),
                                issue_type=category,
                                description=description,
                                severity=severity,
                                line_number=line_num,
                                pattern=pattern,
                            )
                        )

        return issues

    def _analyze_ast_security(
        self, file_path: Path, tree: ast.AST
    ) -> List[SecurityIssue]:
        """Analyse de s√©curit√© via AST"""
        issues = []

        for node in ast.walk(tree):
            # D√©tection d'appels de fonctions dangereuses
            if isinstance(node, ast.Call):
                issues.extend(self._check_dangerous_calls(file_path, node))

            # D√©tection de manipulations de cha√Ænes dangereuses
            if isinstance(node, ast.BinOp):
                issues.extend(self._check_string_operations(file_path, node))

        return issues

    def _check_dangerous_calls(
        self, file_path: Path, call_node: ast.Call
    ) -> List[SecurityIssue]:
        """V√©rifie les appels de fonctions dangereux"""
        issues = []

        # Extraction du nom de la fonction appel√©e
        func_name = ""
        if isinstance(call_node.func, ast.Name):
            func_name = call_node.func.id
        elif isinstance(call_node.func, ast.Attribute):
            func_name = call_node.func.attr

        # V√©rification des fonctions dangereuses
        dangerous_functions = {
            "eval": ("Appel eval() d√©tect√© via AST", "CRITICAL"),
            "exec": ("Appel exec() d√©tect√© via AST", "CRITICAL"),
            "compile": ("Compilation dynamique d√©tect√©e", "HIGH"),
            "system": ("Appel syst√®me d√©tect√©", "CRITICAL"),
        }

        if func_name in dangerous_functions:
            description, severity = dangerous_functions[func_name]
            issues.append(
                SecurityIssue(
                    file_path=str(file_path),
                    issue_type="dangerous_call",
                    description=description,
                    severity=severity,
                    line_number=call_node.lineno,
                    pattern=f"ast_call_{func_name}",
                )
            )

        return issues

    def _check_string_operations(
        self, file_path: Path, binop_node: ast.BinOp
    ) -> List[SecurityIssue]:
        """V√©rifie les op√©rations sur cha√Ænes potentiellement dangereuses"""
        issues = []

        # D√©tection de concat√©nation avec des variables suspectes
        if isinstance(binop_node.op, ast.Add):
            # Recherche de variables nomm√©es "input", "user_input", etc.
            for node in ast.walk(binop_node):
                if isinstance(node, ast.Name) and node.id in [
                    "input",
                    "user_input",
                    "request",
                ]:
                    issues.append(
                        SecurityIssue(
                            file_path=str(file_path),
                            issue_type="string_injection",
                            description="Concat√©nation avec input utilisateur",
                            severity="MEDIUM",
                            line_number=binop_node.lineno,
                            pattern="ast_string_concat",
                        )
                    )
                    break

        return issues

    def _calculate_security_score(
        self, issues: List[SecurityIssue], total_files: int
    ) -> float:
        """Calcule un score de s√©curit√© (0-100, 100 = excellent)"""
        if total_files == 0:
            return 100.0

        # Pond√©ration par s√©v√©rit√©
        severity_weights = {"CRITICAL": 10, "HIGH": 5, "MEDIUM": 2, "LOW": 1}

        # Calcul du score de risque
        risk_score = 0
        for issue in issues:
            risk_score += severity_weights.get(issue.severity, 1)

        # Normalisation (plus il y a de risques, plus le score baisse)
        max_possible_risk = total_files * 5  # Estimation du risque maximum
        normalized_risk = (
            min(risk_score / max_possible_risk, 1.0) if max_possible_risk > 0 else 0
        )

        # Score de s√©curit√© (invers√©)
        security_score = (1 - normalized_risk) * 100

        return round(security_score, 1)

    def generate_security_report(self, result: SecurityScanResult) -> str:
        """G√©n√®re un rapport de s√©curit√© textuel"""
        lines = []
        lines.append("üõ°Ô∏è RAPPORT DE S√âCURIT√â")
        lines.append("=" * 30)

        # Statistiques globales
        lines.append(f"Fichiers scann√©s: {result.scanned_files}")
        lines.append(f"Probl√®mes d√©tect√©s: {len(result.security_issues)}")
        lines.append(f"Score de s√©curit√©: {result.security_score}/100")
        lines.append(f"Fichiers √† haut risque: {len(result.high_risk_files)}")

        # Analyse par s√©v√©rit√©
        if result.security_issues:
            by_severity = {}
            for issue in result.security_issues:
                severity = issue.severity
                if severity not in by_severity:
                    by_severity[severity] = []
                by_severity[severity].append(issue)

            lines.append("\nüö® PROBL√àMES PAR S√âV√âRIT√â:")
            for severity in ["CRITICAL", "HIGH", "MEDIUM", "LOW"]:
                if severity in by_severity:
                    count = len(by_severity[severity])
                    lines.append(f"  {severity}: {count} probl√®me(s)")

                    # Affichage des premiers probl√®mes
                    for issue in by_severity[severity][:3]:
                        filename = Path(issue.file_path).name
                        lines.append(
                            f"    {filename}:{issue.line_number} - {issue.description}"
                        )

        # Verdict de s√©curit√©
        if result.security_score >= 90:
            lines.append("\n‚úÖ EXCELLENT NIVEAU DE S√âCURIT√â")
        elif result.security_score >= 70:
            lines.append("\n‚ö†Ô∏è NIVEAU DE S√âCURIT√â ACCEPTABLE")
        else:
            lines.append("\n‚ùå NIVEAU DE S√âCURIT√â INSUFFISANT")

        return "\n".join(lines)


def quick_security_scan(target_dir: Path) -> bool:
    """Scan rapide de s√©curit√© - fonction utilitaire"""
    validator = SecurityValidator()
    result = validator.scan_directory(target_dir)

    # Consid√®re comme "s√ªr" si pas de probl√®mes critiques
    critical_issues = [i for i in result.security_issues if i.severity == "CRITICAL"]
    return len(critical_issues) == 0


def get_security_summary(target_dir: Path) -> Dict:
    """R√©sum√© de s√©curit√© - fonction utilitaire"""
    validator = SecurityValidator()
    result = validator.scan_directory(target_dir)

    critical_count = len(
        [i for i in result.security_issues if i.severity == "CRITICAL"]
    )
    high_count = len([i for i in result.security_issues if i.severity == "HIGH"])

    return {
        "secure": critical_count == 0 and high_count <= 1,
        "critical_issues": critical_count,
        "high_issues": high_count,
        "security_score": result.security_score,
        "high_risk_files": len(result.high_risk_files),
    }


if __name__ == "__main__":
    # Test du validateur de s√©curit√©
    import sys

    target = Path(sys.argv[1]) if len(sys.argv) > 1 else Path.cwd()

    print("üõ°Ô∏è Test du validateur de s√©curit√© AGI")
    print("=" * 40)

    validator = SecurityValidator()
    result = validator.scan_directory(target)

    print(validator.generate_security_report(result))
