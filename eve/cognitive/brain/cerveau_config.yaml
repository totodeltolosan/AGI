# Configuration pour le module cerveau.py d'ALMA (V20.5.1+)
# Cible : Test "Full Power" avec 32Go RAM et fonctionnalités Core activées

# --- Configuration des Chemins ---
# Suffixes de répertoires par rapport à BASE_ALMA_DIR (ex: /home/toni/Documents/ALMA/)
paths:
  connaissance_dir_suffix: "Connaissance"
  cerveau_dir_suffix: "Cerveau"
  log_dir_suffix: "logs"
  improvements_subdir: "ameliorations_proposees" # Sous-dossier de cerveau_dir_suffix
  active_improvements_dir_suffix: "Connaissance/Amelioree" # Relatif à BASE_ALMA_DIR

# --- Configuration de la Journalisation (Logging) ---
logging:
  log_file_name: "cerveau.log"
  emergency_log_file_name: "cerveau_emergency.log"
  module_registry_file_name: "module_registry.json"
  module_registry_lock_file_name: "module_registry.lock"
  rotation:
    max_bytes: 20971520  # 20MB
    backup_count: 5

# --- Paramètres du Service Cerveau ---
service_params:
  watchdog_enabled: True
  file_scan_interval_seconds: 60 # Réduit pour plus de réactivité après le scan initial
  allowed_file_extensions:
    - ".txt"
    - ".md"
    - ".json"
    - ".xml"
    - ".py"
    - ".sh"
    # Ajouter d'autres extensions si nécessaire (ex: .tex, .java, .c, .cpp, .h)
  max_workers: 7 # Ajusté pour 8 cœurs logiques (7 workers + 1 MainThread) - à confirmer avec votre CPU
  file_queue_max_size: 1000
  self_report_interval_seconds: 600 # Toutes les 10 minutes
  task_timeout_seconds: 1800 # 30 minutes (augmenté pour SBERT/Transformers sur fichiers potentiellement longs)
  backpressure_active_task_multiplier: 3 # (max_workers * 3) tâches max en attente de callback
  health_check_interval_seconds: 180 # Toutes les 3 minutes
  excluded_dir_parts: [
      '.venv', 'venv', 'env', '.env', '__pycache__', 'node_modules',
      '.git', '.hg', '.svn', 'tmp', 'temp', 'backup', 'archives',
      'site-packages', 'dist-packages', # Pour éviter de scanner le venv si mal placé
      'build', 'dist', '.pytest_cache', '.mypy_cache', '.idea', '.vscode'
    ]
  excluded_dir_prefixes: ['.'] # Ignorer tous les dossiers/fichiers cachés

# --- Paramètres du Disjoncteur (Circuit Breaker) ---
circuit_breaker:
  threshold: 3 # Nombre d'échecs avant mise en quarantaine
  timeout_seconds: 3600 # 1 heure de quarantaine

# --- Paramètres NLP (Traitement du Langage Naturel) ---
nlp:
  use_spacy_if_available: True
  spacy_model_names:
    - "fr_core_news_lg" # Priorité au modèle large avec 32Go RAM
    - "fr_core_news_sm" # Fallback
    - "en_core_web_sm"  # Pour l'anglais si détecté
  spacy_max_text_length: 2000000 # 2 millions de caractères (environ 8MB de texte UTF-8)
  sentiment_positive_words:
    - "bon"
    - "excellent"
    - "super"
    - "amélioration"
    - "positif"
    - "correct"
    - "valide"
    - "réussi"
    - "succès"
    - "parfait"
    - "génial"
  sentiment_negative_words:
    - "mauvais"
    - "erreur"
    - "problème"
    - "négatif"
    - "pire"
    - "invalide"
    - "échec"
    - "cassé"
    - "bug"
    - "régression"
    - "difficile"
  significant_pos_tags: # Tags POS considérés comme porteurs de sens
    - "NOUN"    # Nom
    - "PROPN"   # Nom propre
    - "VERB"    # Verbe
    - "ADJ"     # Adjectif
    # - "ADV"   # Adverbe (optionnel, peut ajouter du bruit)
  use_nltk_if_available: True
  use_rake_if_available: True
  use_yake_if_available: True
  use_sklearn_tfidf_if_available: True
  default_language: "fr"

# --- Configuration de la Base de Connaissances (Knowledge Base) ---
knowledge_base:
  use_sqlite_db: True
  db_name: "cerveau_knowledge.sqlite"
  schema_file: "cerveau_kb_schema.sql" # Assurez-vous qu'il contient la colonne 'embedding BLOB'
  db_timeout_seconds: 15 # Légèrement augmenté pour les écritures potentiellement plus longues

# --- Configuration des Étapes du Pipeline de Traitement ---
# L'ordre est géré par la 'priority' (plus petit = exécuté en premier)
pipeline_steps:
  ComprehensionStep:
    enabled: True
    priority: 10
  AnalysisStep:
    enabled: True
    priority: 20
    sentiment_threshold: 0.05
  StudyStep: # Enregistre dans la KB, y compris les embeddings
    enabled: True
    priority: 30
  ImprovementProposalStep: # Utilise FAISS, LanguageTool, Transformers Summarizer
    enabled: True
    priority: 40
  ActiveImprovementStep: # Applique des actions Core
    enabled: True # Mettre à True pour tester les actions automatiques
    priority: 50
    auto_apply_summary: True # METTRE À TRUE pour tester le résumé abstractif automatique
    auto_apply_grammar: True # METTRE À TRUE pour tester LanguageTool automatique

# --- Configuration des Algorithmes du Module Core (Cerveau/Core/core.py) ---
core_algorithms_config:
  text_improver:
    default_language: "fr"
    summary_ratio: 0.15 # Pour le résumé extractif de fallback (si abstractif échoue)
    language_tool_config:
      enabled: true # Important : Activer LanguageTool
      url: "http://localhost:8010/v2/check" # URL de votre serveur LanguageTool
      language: "fr-FR"
      # Optionnel: d'autres paramètres pour LT si votre Core.py les gère
      # ex: motherTongue, preferredVariants, disabledRules, enabledRules
    abstractive_summary_model: "moussaKam/barthez" # Modèle BARThez BASE (165M params)

  knowledge_linker:
    min_similarity_for_link: 0.80 # Seuil pour considérer deux documents comme similaires via FAISS
    num_semantic_links_to_propose: 5 # Nombre de liens similaires à proposer
    sentence_transformer_model: "paraphrase-multilingual-MiniLM-L12-v2" # Modèle pour SBERT/FAISS
