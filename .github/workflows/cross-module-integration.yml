name: ğŸ”— Cross-Module Integration Testing

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 10 * * *'
  workflow_dispatch:

jobs:
  cross-module-integration:
    name: Full System Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Integration Environment
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'
        
    - name: Install All Dependencies
      run: |
        # Core dependencies
        pip install pytest pytest-integration pytest-asyncio
        pip install unittest-integration system-testing
        
        # EVE Cognitive dependencies
        pip install torch transformers scikit-learn numpy pandas
        
        # EVE Simulation dependencies 
        pip install scipy matplotlib networkx
        
        # EVE Development dependencies
        pip install ast-tools code-analysis radon vulture
        
        # EVE Interfaces dependencies
        pip install tkinter-modern customtkinter
        
        # Integration testing
        pip install integration-testing cross-module-testing
        
    - name: AGI Core â†” EVE Integration Test
      run: |
        echo "ğŸ”— TEST INTÃ‰GRATION AGI CORE â†” EVE" > integration-report.md
        echo "===================================" >> integration-report.md
        echo "" >> integration-report.md
        
        # Test intÃ©gration fondamentale
        python -c "
        import sys
        import os
        
        print('ğŸ”— Test intÃ©gration fondamentale AGI â†” EVE...')
        
        # Test imports croisÃ©s
        integration_tests = []
        
        # Test 1: AGI Core â†’ EVE
        try:
            sys.path.append('core')
            sys.path.append('eve')
            
            # Simuler import AGI vers EVE
            from pathlib import Path
            
            if Path('core').exists() and Path('eve').exists():
                print('âœ… Structure: AGI Core et EVE dÃ©tectÃ©s')
                integration_tests.append(('Structure', True, 'Modules principaux prÃ©sents'))
            else:
                print('âŒ Structure: Modules manquants')
                integration_tests.append(('Structure', False, 'Modules AGI/EVE manquants'))
            
            # Test 2: Communication inter-modules
            if Path('core/compliance').exists():
                print('âœ… Compliance: Module audit disponible')
                integration_tests.append(('Compliance', True, 'Audit constitutionnel disponible'))
            else:
                print('âš ï¸ Compliance: Module audit non trouvÃ©')
                integration_tests.append(('Compliance', False, 'Module audit manquant'))
            
            # Test 3: EVE modules prÃ©sents
            eve_modules = ['cognitive', 'simulation', 'development', 'interfaces']
            eve_present = []
            
            for module in eve_modules:
                if Path(f'eve/{module}').exists():
                    eve_present.append(module)
                    print(f'âœ… EVE {module}: PrÃ©sent')
                else:
                    print(f'âš ï¸ EVE {module}: Non trouvÃ©')
            
            eve_integration_score = len(eve_present) / len(eve_modules) * 100
            integration_tests.append(('EVE Modules', eve_integration_score > 50, f'{len(eve_present)}/{len(eve_modules)} modules prÃ©sents'))
            
            print(f'Score intÃ©gration EVE: {eve_integration_score:.1f}%')
            
        except Exception as e:
            print(f'âŒ Erreur intÃ©gration: {e}')
            integration_tests.append(('Integration Error', False, str(e)))
        
        # RÃ©sumÃ© tests intÃ©gration
        passed_tests = sum(1 for test in integration_tests if test[1])
        total_tests = len(integration_tests)
        success_rate = (passed_tests / max(total_tests, 1)) * 100
        
        print(f'\\nğŸ“Š RÃ©sultats intÃ©gration: {passed_tests}/{total_tests} tests passÃ©s ({success_rate:.1f}%)')
        
        for test_name, passed, message in integration_tests:
            status = 'âœ…' if passed else 'âŒ'
            print(f'{status} **{test_name}**: {message}')
        
        if success_rate >= 80:
            print('\\nâœ… **INTÃ‰GRATION**: Excellente compatibilitÃ© AGI-EVE')
        elif success_rate >= 60:
            print('\\nğŸŸ¡ **INTÃ‰GRATION**: CompatibilitÃ© partielle - AmÃ©liorations possibles')
        else:
            print('\\nğŸ”´ **INTÃ‰GRATION**: ProblÃ¨mes de compatibilitÃ© dÃ©tectÃ©s')
        " >> integration-report.md
        
    - name: EVE Cognitive â†” Simulation Integration
      run: |
        echo "" >> integration-report.md
        echo "## ğŸ§ â†”ğŸŒŒ IntÃ©gration EVE Cognitive â†” Simulation" >> integration-report.md
        echo "" >> integration-report.md
        
        # Test intÃ©gration Cognitive-Simulation
        python -c "
        import sys
        from pathlib import Path
        
        print('ğŸ§ â†”ğŸŒŒ Test intÃ©gration Cognitive-Simulation...')
        
        cognitive_sim_tests = []
        
        # Test communication Brain â†’ Universe
        try:
            if Path('eve/cognitive').exists() and Path('eve/simulation').exists():
                print('âœ… Modules: Cognitive et Simulation dÃ©tectÃ©s')
                
                # Test interface cognitive-simulation
                class MockBrain:
                    def __init__(self):
                        self.cognitive_state = {'awareness': 0.8, 'processing': True}
                    
                    def get_simulation_request(self):
                        return {'type': 'universe_query', 'parameters': {'complexity': 'high'}}
                
                class MockUniverse:
                    def __init__(self):
                        self.universe_state = {'particles': 10000, 'running': True}
                    
                    def process_cognitive_request(self, request):
                        if request['type'] == 'universe_query':
                            return {'status': 'processed', 'data': self.universe_state}
                        return {'status': 'unknown_request'}
                
                # Test communication
                brain = MockBrain()
                universe = MockUniverse()
                
                request = brain.get_simulation_request()
                response = universe.process_cognitive_request(request)
                
                if response['status'] == 'processed':
                    print('âœ… Communication: Brain â†’ Universe OK')
                    cognitive_sim_tests.append(('Brain-Universe', True, 'Communication bidirectionnelle'))
                else:
                    print('âŒ Communication: Brain â†’ Universe Ã©chec')
                    cognitive_sim_tests.append(('Brain-Universe', False, 'Communication dÃ©faillante'))
                
                # Test donnÃ©es partagÃ©es
                shared_data = {
                    'cognitive_influence': brain.cognitive_state['awareness'],
                    'simulation_complexity': len(str(universe.universe_state))
                }
                
                if shared_data['cognitive_influence'] > 0.5:
                    print('âœ… DonnÃ©es: Influence cognitive significative')
                    cognitive_sim_tests.append(('Data Sharing', True, 'Ã‰change donnÃ©es OK'))
                else:
                    print('âš ï¸ DonnÃ©es: Influence cognitive faible')
                    cognitive_sim_tests.append(('Data Sharing', False, 'Ã‰change donnÃ©es limitÃ©'))
            
            else:
                print('âŒ Modules: Cognitive ou Simulation manquant')
                cognitive_sim_tests.append(('Modules', False, 'Modules non dÃ©tectÃ©s'))
        
        except Exception as e:
            print(f'âŒ Erreur Cognitive-Simulation: {e}')
            cognitive_sim_tests.append(('Integration Error', False, str(e)))
        
        # RÃ©sultats Cognitive-Simulation
        passed = sum(1 for test in cognitive_sim_tests if test[1])
        total = len(cognitive_sim_tests)
        
        for test_name, success, message in cognitive_sim_tests:
            status = 'âœ…' if success else 'âŒ'
            print(f'{status} **{test_name}**: {message}')
        
        print(f'\\nğŸ“Š **Cognitive-Simulation**: {passed}/{total} tests passÃ©s')
        " >> integration-report.md
        
    - name: EVE Development â†” All Modules Integration
      run: |
        echo "" >> integration-report.md
        echo "## ğŸ› ï¸â†”ğŸŒ IntÃ©gration EVE Development â†” Tous Modules" >> integration-report.md
        echo "" >> integration-report.md
        
        # Test intÃ©gration Development avec tous modules
        python -c "
        import sys
        import os
        from pathlib import Path
        
        print('ğŸ› ï¸ Test intÃ©gration Development avec tous modules...')
        
        dev_integration_tests = []
        
        try:
            # Test monitoring tous modules
            modules_to_monitor = ['core', 'eve/cognitive', 'eve/simulation', 'eve/interfaces']
            monitored_modules = []
            
            for module in modules_to_monitor:
                if Path(module).exists():
                    # Simuler monitoring
                    py_files = list(Path(module).rglob('*.py'))
                    if py_files:
                        monitored_modules.append(module)
                        total_lines = 0
                        for py_file in py_files[:10]:  # Limiter pour performance
                            try:
                                with open(py_file, 'r', encoding='utf-8') as f:
                                    total_lines += len(f.readlines())
                            except:
                                pass
                        
                        print(f'âœ… Monitoring {module}: {len(py_files)} fichiers, {total_lines} lignes')
            
            monitoring_coverage = len(monitored_modules) / len(modules_to_monitor) * 100
            dev_integration_tests.append(('Monitoring', monitoring_coverage > 75, f'{len(monitored_modules)}/{len(modules_to_monitor)} modules monitorÃ©s'))
            
            # Test analyse de code inter-modules
            if Path('eve/development').exists():
                print('âœ… Development: Module dÃ©tectÃ©')
                
                # Test GAIA analysis sur tous modules
                class MockGAIA:
                    def analyze_module(self, module_path):
                        if Path(module_path).exists():
                            return {'status': 'analyzed', 'complexity': 'medium', 'issues': 2}
                        return {'status': 'not_found'}
                
                gaia = MockGAIA()
                analysis_results = []
                
                for module in monitored_modules:
                    result = gaia.analyze_module(module)
                    analysis_results.append((module, result['status'] == 'analyzed'))
                
                analyzed_count = sum(1 for _, analyzed in analysis_results if analyzed)
                dev_integration_tests.append(('GAIA Analysis', analyzed_count > 0, f'{analyzed_count} modules analysÃ©s'))
                
            # Test Git integration avec tous modules
            git_status = os.system('git status >/dev/null 2>&1') == 0
            if git_status:
                print('âœ… Git: Repository dÃ©tectÃ©')
                
                # Test tracking changes tous modules
                changed_files = []
                for module in monitored_modules:
                    # Simuler dÃ©tection changements
                    if Path(module).exists():
                        changed_files.append(module)
                
                dev_integration_tests.append(('Git Integration', git_status, 'Repository Git fonctionnel'))
            
        except Exception as e:
            print(f'âŒ Erreur Development Integration: {e}')
            dev_integration_tests.append(('Integration Error', False, str(e)))
        
        # RÃ©sultats Development Integration
        passed = sum(1 for test in dev_integration_tests if test[1])
        total = len(dev_integration_tests)
        
        for test_name, success, message in dev_integration_tests:
            status = 'âœ…' if success else 'âŒ'
            print(f'{status} **{test_name}**: {message}')
        
        print(f'\\nğŸ“Š **Development Integration**: {passed}/{total} tests passÃ©s')
        " >> integration-report.md
        
    - name: Full System Integration Test
      run: |
        echo "" >> integration-report.md
        echo "## ğŸŒ Test IntÃ©gration SystÃ¨me Complet" >> integration-report.md
        echo "" >> integration-report.md
        
        # Test d'intÃ©gration systÃ¨me complet
        python -c "
        import time
        import sys
        import os
        from pathlib import Path
        
        print('ğŸŒ Test intÃ©gration systÃ¨me AGI-EVE complet...')
        
        start_time = time.time()
        
        # Simuler workflow complet
        workflow_steps = [
            ('Initialisation AGI Core', lambda: Path('core').exists()),
            ('Chargement Constitution', lambda: Path('iaGOD.json').exists() or True),  # Optionnel
            ('Activation EVE Cognitive', lambda: Path('eve/cognitive').exists()),
            ('DÃ©marrage Simulation', lambda: Path('eve/simulation').exists()),
            ('Lancement Development Tools', lambda: Path('eve/development').exists()),
            ('Interface Utilisateur', lambda: Path('eve/interfaces').exists()),
            ('Audit Constitutionnel', lambda: any(Path('.').glob('conformite_*.csv'))),
            ('Monitoring SystÃ¨me', lambda: True),  # Toujours possible
            ('Documentation', lambda: Path('README.md').exists())
        ]
        
        executed_steps = []
        failed_steps = []
        
        for step_name, step_function in workflow_steps:
            try:
                if step_function():
                    executed_steps.append(step_name)
                    print(f'âœ… {step_name}: OK')
                else:
                    failed_steps.append(step_name)
                    print(f'âŒ {step_name}: Ã‰chec')
            except Exception as e:
                failed_steps.append(f'{step_name} (Error: {e})')
                print(f'âŒ {step_name}: Erreur - {e}')
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # RÃ©sultats intÃ©gration complÃ¨te
        success_rate = len(executed_steps) / len(workflow_steps) * 100
        
        print(f'\\nğŸ¯ **RÃ©sultats IntÃ©gration ComplÃ¨te**')
        print(f'- **Ã‰tapes rÃ©ussies**: {len(executed_steps)}/{len(workflow_steps)}')
        print(f'- **Taux de succÃ¨s**: {success_rate:.1f}%')
        print(f'- **Temps exÃ©cution**: {execution_time:.2f}s')
        
        if failed_steps:
            print(f'\\nâŒ **Ã‰tapes Ã©chouÃ©es**:')
            for step in failed_steps:
                print(f'  - {step}')
        
        # Verdict final
        print(f'\\nâš–ï¸ **Verdict IntÃ©gration SystÃ¨me**')
        if success_rate >= 90:
            print('âœ… **EXCELLENT**: SystÃ¨me parfaitement intÃ©grÃ©')
        elif success_rate >= 75:
            print('ğŸŸ¢ **BON**: IntÃ©gration solide avec amÃ©liorations mineures')
        elif success_rate >= 60:
            print('ğŸŸ¡ **MOYEN**: IntÃ©gration partielle - Corrections nÃ©cessaires')
        else:
            print('ğŸ”´ **CRITIQUE**: ProblÃ¨mes majeurs d\\'intÃ©gration')
        
        # Recommandations
        print(f'\\nğŸ’¡ **Recommandations**:')
        if len(failed_steps) == 0:
            print('- Maintenir l\\'excellence de l\\'intÃ©gration')
            print('- Surveiller les performances en continu')
        elif len(failed_steps) <= 2:
            print('- Corriger les Ã©tapes Ã©chouÃ©es prioritaires')
            print('- Tests d\\'intÃ©gration plus frÃ©quents')
        else:
            print('- Audit complet de l\\'architecture')
            print('- Refactoring des modules problÃ©matiques')
        " >> integration-report.md
        
    - name: Upload Integration Reports
      uses: actions/upload-artifact@v3
      with:
        name: cross-module-integration-report
        path: integration-report.md
