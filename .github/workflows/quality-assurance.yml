name:  Quality Assurance

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 16 * * *'  # Quotidien 16h
  workflow_dispatch:

jobs:
  quality-assurance:
    name: Comprehensive Quality Assurance
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup QA Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install QA Tools
      run: |
        pip install quality-assurance testing-framework
        pip install pylint flake8 black isort mypy
        pip install pytest pytest-cov pytest-xdist
        pip install bandit safety vulture radon
        pip install code-quality-tools qa-automation
        
    - name: Code Quality Assessment
      run: |
        echo "ASSURANCE QUALIT COMPLTE AGI-EVE" > qa-report.md
        echo "====================================" >> qa-report.md
        echo "" >> qa-report.md
        
        # Assessment qualit globale
        python -c "
        import subprocess
        import json
        import os
        from pathlib import Path
        
        print(' Assessment qualit code complet...')
        
        qa_metrics = {
            'style_compliance': {},
            'code_complexity': {},
            'security_issues': {},
            'test_coverage': {},
            'documentation': {},
            'performance': {}
        }
        
        # 1. Style et conformit
        print(' Analyse style et conformit...')
        
        # Black formatting check
        try:
            result = subprocess.run(['black', '--check', '--diff', '.'], 
                                  capture_output=True, text=True)
            qa_metrics['style_compliance']['black'] = {
                'status': 'PASS' if result.returncode == 0 else 'FAIL',
                'issues': len(result.stdout.splitlines()) if result.stdout else 0
            }
            print(f'- **Black formatting**: {qa_metrics[\"style_compliance\"][\"black\"][\"status\"]}')
        except:
            qa_metrics['style_compliance']['black'] = {'status': 'ERROR', 'issues': 0}
        
        # Import sorting check
        try:
            result = subprocess.run(['isort', '--check-only', '--diff', '.'], 
                                  capture_output=True, text=True)
            qa_metrics['style_compliance']['isort'] = {
                'status': 'PASS' if result.returncode == 0 else 'FAIL',
                'issues': len(result.stdout.splitlines()) if result.stdout else 0
            }
            print(f'- **Import sorting**: {qa_metrics[\"style_compliance\"][\"isort\"][\"status\"]}')
        except:
            qa_metrics['style_compliance']['isort'] = {'status': 'ERROR', 'issues': 0}
        
        # Flake8 linting
        try:
            result = subprocess.run(['flake8', '.', '--count'], 
                                  capture_output=True, text=True)
            issues_count = int(result.stdout.strip()) if result.stdout.strip().isdigit() else 0
            qa_metrics['style_compliance']['flake8'] = {
                'status': 'PASS' if issues_count == 0 else 'FAIL',
                'issues': issues_count
            }
            print(f'- **Flake8 linting**: {qa_metrics[\"style_compliance\"][\"flake8\"][\"status\"]} ({issues_count} issues)')
        except:
            qa_metrics['style_compliance']['flake8'] = {'status': 'ERROR', 'issues': 0}
        
        # 2. Complexit code
        print('\\n Analyse complexit code...')
        
        # Radon complexity
        try:
            result = subprocess.run(['radon', 'cc', '.', '-a', '--total-average'], 
                                  capture_output=True, text=True)
            
            # Parser sortie radon pour extraire complexit moyenne
            lines = result.stdout.splitlines()
            avg_complexity = 0
            high_complexity_count = 0
            
            for line in lines:
                if 'Average complexity:' in line:
                    try:
                        avg_complexity = float(line.split(':')[1].strip().split()[0])
                    except:
                        pass
                elif any(grade in line for grade in ['C', 'D', 'E', 'F']):
                    high_complexity_count += 1
            
            qa_metrics['code_complexity']['radon'] = {
                'average_complexity': avg_complexity,
                'high_complexity_functions': high_complexity_count,
                'status': 'PASS' if avg_complexity < 5 and high_complexity_count < 10 else 'FAIL'
            }
            print(f'- **Complexit moyenne**: {avg_complexity:.2f}')
            print(f'- **Fonctions complexes**: {high_complexity_count}')
        except:
            qa_metrics['code_complexity']['radon'] = {'status': 'ERROR'}
        
        # 3. Analyse scurit
        print('\\n Analyse scurit...')
        
        # Bandit security scan
        try:
            result = subprocess.run(['bandit', '-r', '.', '-f', 'json'], 
                                  capture_output=True, text=True)
            
            if result.stdout:
                bandit_data = json.loads(result.stdout)
                high_severity = len([r for r in bandit_data.get('results', []) if r.get('issue_severity') == 'HIGH'])
                medium_severity = len([r for r in bandit_data.get('results', []) if r.get('issue_severity') == 'MEDIUM'])
                
                qa_metrics['security_issues']['bandit'] = {
                    'high_severity': high_severity,
                    'medium_severity': medium_severity,
                    'total_issues': len(bandit_data.get('results', [])),
                    'status': 'PASS' if high_severity == 0 else 'FAIL'
                }
                print(f'- **Scurit Bandit**: {high_severity} high, {medium_severity} medium')
        except:
            qa_metrics['security_issues']['bandit'] = {'status': 'ERROR'}
        
        # Safety dependency check
        try:
            result = subprocess.run(['safety', 'check', '--json'], 
                                  capture_output=True, text=True)
            
            if result.stdout:
                safety_data = json.loads(result.stdout)
                vulnerabilities = len(safety_data) if isinstance(safety_data, list) else 0
                
                qa_metrics['security_issues']['safety'] = {
                    'vulnerabilities': vulnerabilities,
                    'status': 'PASS' if vulnerabilities == 0 else 'FAIL'
                }
                print(f'- **Vulnrabilits dpendances**: {vulnerabilities}')
        except:
            qa_metrics['security_issues']['safety'] = {'status': 'ERROR'}
        
        # Sauvegarder mtriques QA
        with open('qa_metrics.json', 'w') as f:
            json.dump(qa_metrics, f, indent=2)
        
        print('\\n Mtriques QA sauvegardes: qa_metrics.json')
        " >> qa-report.md
        
    - name: Test Coverage Analysis
      run: |
        echo "" >> qa-report.md
        echo "## Analyse Couverture Tests" >> qa-report.md
        echo "" >> qa-report.md
        
        # Analyse couverture tests
        python -c "
        import subprocess
        import os
        from pathlib import Path
        
        print(' Analyse couverture tests...')
        
        # Dcouverte tests
        test_files = list(Path('.').rglob('test_*.py')) + list(Path('.').rglob('*_test.py'))
        test_dirs = [d for d in Path('.').rglob('tests') if d.is_dir()]
        
        print(f'- **Fichiers tests**: {len(test_files)} dtects')
        print(f'- **Dossiers tests**: {len(test_dirs)} dtects')
        
        if test_files or test_dirs:
            # Excuter tests avec couverture
            try:
                result = subprocess.run([
                    'pytest', '--cov=.', '--cov-report=term', '--cov-report=xml',
                    '--cov-report=html', '-v'
                ], capture_output=True, text=True, timeout=300)
                
                # Parser rsultats couverture
                coverage_line = ''
                for line in result.stdout.splitlines():
                    if 'TOTAL' in line and '%' in line:
                        coverage_line = line
                        break
                
                if coverage_line:
                    # Extraire pourcentage couverture
                    parts = coverage_line.split()
                    coverage_percent = 0
                    for part in parts:
                        if '%' in part:
                            try:
                                coverage_percent = int(part.replace('%', ''))
                                break
                            except:
                                pass
                    
                    print(f'- **Couverture totale**: {coverage_percent}%')
                    
                    # valuation couverture
                    if coverage_percent >= 90:
                        print(' **Couverture excellente**: >90%')
                    elif coverage_percent >= 70:
                        print(' **Couverture bonne**: 70-90%')
                    elif coverage_percent >= 50:
                        print(' **Couverture moyenne**: 50-70%')
                    else:
                        print(' **Couverture insuffisante**: <50%')
                else:
                    print(' **Couverture**: Impossible de parser rsultats')
                
                # Tests passs/chous
                if 'failed' in result.stdout:
                    print(' **Tests**: Certains tests chouent')
                elif 'passed' in result.stdout:
                    print(' **Tests**: Tous les tests passent')
                
            except subprocess.TimeoutExpired:
                print(' **Tests**: Timeout - Tests trop longs')
            except Exception as e:
                print(f' **Tests**: Erreur excution - {e}')
        else:
            print(' **Tests**: Aucun test trouv')
            print(' **Recommandation**: Crer suite de tests complte')
            print('  - Tests unitaires pour chaque module')
            print('  - Tests d\\'intgration AGI-EVE')
            print('  - Tests performance critiques')
        " >> qa-report.md
        
    - name: Documentation Quality Check
      run: |
        echo "" >> qa-report.md
        echo "##  Qualit Documentation" >> qa-report.md
        echo "" >> qa-report.md
        
        # valuation qualit documentation
        python -c "
        import ast
        import os
        import re
        from pathlib import Path
        
        print(' valuation qualit documentation...')
        
        doc_quality = {
            'readme_quality': 0,
            'docstring_coverage': 0,
            'comment_density': 0,
            'api_documentation': 0,
            'examples_provided': 0
        }
        
        # 1. Qualit README
        readme_files = [f for f in Path('.').glob('README*') if f.is_file()]
        if readme_files:
            readme_path = readme_files[0]
            try:
                with open(readme_path, 'r', encoding='utf-8') as f:
                    readme_content = f.read()
                
                readme_score = 0
                # Critres qualit README
                criteria = [
                    ('Description projet', r'description|overview|about'),
                    ('Installation', r'install|setup|getting started'),
                    ('Usage/Examples', r'usage|example|how to'),
                    ('Configuration', r'config|configuration|setup'),
                    ('API/Documentation', r'api|documentation|docs'),
                    ('Contributing', r'contribut|development|build'),
                    ('License', r'license|licence'),
                    ('Contact/Support', r'contact|support|help')
                ]
                
                for criterion, pattern in criteria:
                    if re.search(pattern, readme_content, re.IGNORECASE):
                        readme_score += 1
                
                doc_quality['readme_quality'] = (readme_score / len(criteria)) * 100
                print(f'- **README qualit**: {doc_quality[\"readme_quality\"]:.1f}% ({readme_score}/{len(criteria)} sections)')
                
            except Exception as e:
                print(f' **README**: Erreur lecture - {e}')
        else:
            print(' **README**: Fichier README manquant')
        
        # 2. Couverture docstrings
        total_functions = 0
        documented_functions = 0
        total_classes = 0
        documented_classes = 0
        
        for py_file in Path('.').rglob('*.py'):
            if '.git' in str(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    tree = ast.parse(f.read())
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef) and not node.name.startswith('_'):
                        total_functions += 1
                        if ast.get_docstring(node):
                            documented_functions += 1
                    elif isinstance(node, ast.ClassDef):
                        total_classes += 1
                        if ast.get_docstring(node):
                            documented_classes += 1
            except:
                pass
        
        if total_functions > 0:
            func_doc_rate = (documented_functions / total_functions) * 100
            doc_quality['docstring_coverage'] = func_doc_rate
            print(f'- **Docstrings fonctions**: {func_doc_rate:.1f}% ({documented_functions}/{total_functions})')
        
        if total_classes > 0:
            class_doc_rate = (documented_classes / total_classes) * 100
            print(f'- **Docstrings classes**: {class_doc_rate:.1f}% ({documented_classes}/{total_classes})')
        
        # 3. Densit commentaires
        total_lines = 0
        comment_lines = 0
        
        for py_file in Path('.').rglob('*.py'):
            if '.git' in str(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    total_lines += len(lines)
                    
                    for line in lines:
                        stripped = line.strip()
                        if stripped.startswith('#') and not stripped.startswith('#!/'):
                            comment_lines += 1
            except:
                pass
        
        if total_lines > 0:
            comment_density = (comment_lines / total_lines) * 100
            doc_quality['comment_density'] = comment_density
            print(f'- **Densit commentaires**: {comment_density:.1f}% ({comment_lines}/{total_lines} lignes)')
        
        # 4. Documentation API
        api_docs = list(Path('.').rglob('*api*.md')) + list(Path('.').rglob('docs/**/*.md'))
        doc_quality['api_documentation'] = min(len(api_docs) * 25, 100)  # Max 100% avec 4+ docs
        print(f'- **Documentation API**: {len(api_docs)} fichiers trouvs')
        
        # 5. Exemples fournis
        example_files = list(Path('.').rglob('example*')) + list(Path('.').rglob('*example*'))
        doc_quality['examples_provided'] = min(len(example_files) * 20, 100)  # Max 100% avec 5+ exemples
        print(f'- **Fichiers exemples**: {len(example_files)} trouvs')
        
        # Score global documentation
        overall_doc_score = sum(doc_quality.values()) / len(doc_quality)
        print(f'\\n **Score documentation global**: {overall_doc_score:.1f}%')
        
        # Recommandations documentation
        print(f'\\n **Recommandations Documentation**')
        if overall_doc_score >= 80:
            print(' **Documentation excellente**: Maintenir qualit')
        elif overall_doc_score >= 60:
            print(' **Documentation correcte**: Amliorations cibles')
            if doc_quality['docstring_coverage'] < 70:
                print('  - Augmenter couverture docstrings')
            if doc_quality['api_documentation'] < 50:
                print('  - Crer documentation API complte')
        else:
            print(' **Documentation insuffisante**: Amlioration majeure requise')
            print('  - Crer/amliorer README complet')
            print('  - Documenter toutes fonctions publiques')
            print('  - Ajouter exemples d\\'utilisation')
            print('  - Crer documentation API')
        " >> qa-report.md
        
    - name: Overall Quality Score
      run: |
        echo "" >> qa-report.md
        echo "##  Score Qualit Global" >> qa-report.md
        echo "" >> qa-report.md
        
        # Calcul score qualit global
        python -c "
        import json
        import os
        
        print(' Calcul score qualit global...')
        
        # Charger mtriques QA si disponible
        qa_metrics = {}
        if os.path.exists('qa_metrics.json'):
            with open('qa_metrics.json', 'r') as f:
                qa_metrics = json.load(f)
        
        # Systme de notation pondre
        quality_weights = {
            'style_compliance': 20,      # 20% - Important pour maintenabilit
            'code_complexity': 25,       # 25% - Critique pour maintenance
            'security_issues': 30,       # 30% - Essentiel pour production
            'test_coverage': 15,         # 15% - Important pour fiabilit
            'documentation': 10          # 10% - Utile pour adoption
        }
        
        quality_scores = {}
        
        # 1. Score conformit style
        style_score = 0
        style_metrics = qa_metrics.get('style_compliance', {})
        
        for tool, result in style_metrics.items():
            if result.get('status') == 'PASS':
                style_score += 33.33  # 100/3 pour 3 outils
        
        quality_scores['style_compliance'] = min(style_score, 100)
        
        # 2. Score complexit
        complexity_metrics = qa_metrics.get('code_complexity', {})
        radon_data = complexity_metrics.get('radon', {})
        
        if radon_data.get('status') == 'PASS':
            complexity_score = 100
        elif 'average_complexity' in radon_data:
            # Score bas sur complexit moyenne (idal: <3, acceptable: <5, mauvais: >10)
            avg_complexity = radon_data['average_complexity']
            if avg_complexity <= 3:
                complexity_score = 100
            elif avg_complexity <= 5:
                complexity_score = 80
            elif avg_complexity <= 8:
                complexity_score = 60
            else:
                complexity_score = 30
        else:
            complexity_score = 50  # Score neutre si pas de donnes
        
        quality_scores['code_complexity'] = complexity_score
        
        # 3. Score scurit
        security_metrics = qa_metrics.get('security_issues', {})
        security_score = 100
        
        bandit_data = security_metrics.get('bandit', {})
        if bandit_data.get('high_severity', 0) > 0:
            security_score -= bandit_data['high_severity'] * 20  # -20 par vulnrabilit haute
        if bandit_data.get('medium_severity', 0) > 0:
            security_score -= bandit_data['medium_severity'] * 5   # -5 par vulnrabilit moyenne
        
        safety_data = security_metrics.get('safety', {})
        if safety_data.get('vulnerabilities', 0) > 0:
            security_score -= safety_data['vulnerabilities'] * 15  # -15 par vulnrabilit dpendance
        
        quality_scores['security_issues'] = max(security_score, 0)
        
        # 4. Score tests (simul -  remplacer par vraie couverture)
        # Pour demo, on estime bas sur prsence fichiers tests
        from pathlib import Path
        test_files = list(Path('.').rglob('test_*.py')) + list(Path('.').rglob('*_test.py'))
        py_files = list(Path('.').rglob('*.py'))
        
        if py_files:
            test_ratio = len(test_files) / len(py_files)
            test_score = min(test_ratio * 200, 100)  # 1 test pour 2 fichiers = 100%
        else:
            test_score = 0
        
        quality_scores['test_coverage'] = test_score
        
        # 5. Score documentation (calcul prcdemment, ici estim)
        readme_exists = any(Path('.').glob('README*'))
        doc_files = list(Path('.').rglob('*.md'))
        
        doc_score = 0
        if readme_exists:
            doc_score += 50
        doc_score += min(len(doc_files) * 10, 50)  # Max 50% pour fichiers docs
        
        quality_scores['documentation'] = doc_score
        
        # Calcul score global pondr
        weighted_score = 0
        for category, score in quality_scores.items():
            weight = quality_weights.get(category, 0)
            weighted_score += (score * weight / 100)
        
        # Affichage dtaill
        print('###  Scores par Catgorie')
        for category, score in quality_scores.items():
            weight = quality_weights[category]
            category_name = category.replace('_', ' ').title()
            
            if score >= 90:
                status = ' EXCELLENT'
            elif score >= 70:
                status = ' BON'
            elif score >= 50:
                status = ' MOYEN'
            else:
                status = ' FAIBLE'
            
            print(f'- **{category_name}**: {score:.1f}% (Poids: {weight}%) - {status}')
        
        print(f'\\n **SCORE QUALIT GLOBAL**: {weighted_score:.1f}%')
        
        # Classification qualit
        if weighted_score >= 90:
            quality_grade = 'A+ (EXCELLENCE)'
            quality_icon = ''
        elif weighted_score >= 80:
            quality_grade = 'A (TRS BON)'
            quality_icon = ''
        elif weighted_score >= 70:
            quality_grade = 'B (BON)'
            quality_icon = ''
        elif weighted_score >= 60:
            quality_grade = 'C (ACCEPTABLE)'
            quality_icon = ''
        elif weighted_score >= 50:
            quality_grade = 'D (INSUFFISANT)'
            quality_icon = ''
        else:
            quality_grade = 'F (CRITIQUE)'
            quality_icon = ''
        
        print(f'{quality_icon} **GRADE QUALIT**: {quality_grade}')
        
        # Plan d'amlioration
        print(f'\\n **Plan d\\'Amlioration Prioritaire**')
        
        # Identifier points faibles
        weak_areas = [(cat, score) for cat, score in quality_scores.items() if score < 70]
        weak_areas.sort(key=lambda x: quality_weights[x[0]], reverse=True)  # Trier par importance
        
        if weak_areas:
            print('**Actions Prioritaires** (par ordre d\\'importance):')
            for i, (category, score) in enumerate(weak_areas[:3], 1):
                category_name = category.replace('_', ' ').title()
                print(f'{i}. **{category_name}** ({score:.1f}%) - Poids: {quality_weights[category]}%')
        else:
            print(' **Excellente qualit** - Maintenir standards actuels')
        
        # Objectifs chiffrs
        if weighted_score < 80:
            target_score = min(weighted_score + 20, 90)
            print(f'\\n **Objectif**: Atteindre {target_score:.0f}% dans les 4 semaines')
        
        # Sauvegarder score final
        final_results = {
            'overall_score': weighted_score,
            'grade': quality_grade,
            'category_scores': quality_scores,
            'weak_areas': weak_areas
        }
        
        with open('quality_assessment.json', 'w') as f:
            json.dump(final_results, f, indent=2)
        
        print('\\n Assessment qualit sauvegard: quality_assessment.json')
        " >> qa-report.md
        
    - name: Create Quality Issues
      run: |
        # Crer issues pour problmes qualit critiques
        if [ -f "quality_assessment.json" ]; then
          python -c "
          import json
          
          with open('quality_assessment.json', 'r') as f:
              results = json.load(f)
          
          overall_score = results['overall_score']
          
          if overall_score < 60:
              print('Creating critical quality issue...')
              exit(1)  # Dclencher cration issue
          "
          
          if [ $? -eq 1 ]; then
            gh issue create \
              --title " QUALIT CRITIQUE - Score: $(python -c "import json; print(json.load(open('quality_assessment.json'))['overall_score'])" | head -1)%" \
              --body-file qa-report.md \
              --label "quality,critical,improvement" \
              --assignee "@me" || echo "Issue creation failed"
          fi
        fi
        
    - name: Upload QA Reports
      uses: actions/upload-artifact@v4
      with:
        name: quality-assurance-report
        path: |
          qa-report.md
          qa_metrics.json
          quality_assessment.json
          coverage.xml
          htmlcov/
