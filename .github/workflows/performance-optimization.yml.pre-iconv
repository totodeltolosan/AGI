name:  Performance Optimization

on:
  push:
    branches: [main]
  schedule:
    - cron: '0 14 * * *'  # Quotidien 14h
  workflow_dispatch:

jobs:
  performance-optimization:
    name: System Performance Analysis & Optimization
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Performance Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Performance Tools
      run: |
        pip install performance-testing memory-profiler line-profiler
        pip install psutil py-spy pytest-benchmark cProfile
        pip install optimization-tools system-profiling
        
    - name: System Performance Baseline
      run: |
        echo "ANALYSE PERFORMANCE SYST√àME AGI-EVE" > performance-report.md
        echo "=====================================" >> performance-report.md
        echo "" >> performance-report.md
        
        # Baseline performance syst√®me
        python -c "
        import psutil
        import time
        import os
        from pathlib import Path
        
        print(' √âtablissement baseline performance...')
        
        # M√©triques syst√®me initiales
        initial_stats = {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'processes': len(psutil.pids()),
            'boot_time': psutil.boot_time()
        }
        
        print(f'##  Baseline Syst√®me')
        print(f'- **CPU**: {initial_stats[\"cpu_percent\"]}%')
        print(f'- **M√©moire**: {initial_stats[\"memory_percent\"]}%')
        print(f'- **Disque**: {initial_stats[\"disk_usage\"]}%')
        print(f'- **Processus**: {initial_stats[\"processes\"]}')
        print('')
        
        # Analyse taille projet
        project_stats = {
            'total_files': 0,
            'python_files': 0,
            'total_size': 0,
            'largest_files': []
        }
        
        for root, dirs, files in os.walk('.'):
            if '.git' in root:
                continue
            
            for file in files:
                filepath = Path(root) / file
                try:
                    size = filepath.stat().st_size
                    project_stats['total_files'] += 1
                    project_stats['total_size'] += size
                    
                    if file.endswith('.py'):
                        project_stats['python_files'] += 1
                    
                    # Tracker gros fichiers
                    if size > 100000:  # > 100KB
                        project_stats['largest_files'].append((str(filepath), size))
                except:
                    pass
        
        # Trier gros fichiers
        project_stats['largest_files'].sort(key=lambda x: x[1], reverse=True)
        
        print(f'## üìÅ Taille Projet')
        print(f'- **Total fichiers**: {project_stats[\"total_files\"]}')
        print(f'- **Fichiers Python**: {project_stats[\"python_files\"]}')
        print(f'- **Taille totale**: {project_stats[\"total_size\"] / 1024 / 1024:.1f} MB')
        
        if project_stats['largest_files']:
            print(f'- **Gros fichiers** (>100KB):')
            for filepath, size in project_stats['largest_files'][:5]:
                print(f'  - {filepath}: {size / 1024:.1f} KB')
        print('')
        " >> performance-report.md
        
    - name: Python Code Performance Analysis
      run: |
        echo "" >> performance-report.md
        echo "## üêç Performance Code Python" >> performance-report.md
        echo "" >> performance-report.md
        
        # Analyse performance code Python
        python -c "
        import time
        import cProfile
        import pstats
        import io
        from pathlib import Path
        import ast
        
        print('üêç Analyse performance code Python...')
        
        # M√©triques complexit√© et performance
        performance_stats = {
            'total_lines': 0,
            'total_functions': 0,
            'complex_functions': [],
            'long_files': [],
            'import_analysis': {}
        }
        
        # Analyser complexit√© code
        for py_file in Path('.').rglob('*.py'):
            if '.git' in str(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = len(content.splitlines())
                    performance_stats['total_lines'] += lines
                
                # Fichiers longs (impact performance)
                if lines > 500:
                    performance_stats['long_files'].append((str(py_file), lines))
                
                # Analyse AST pour complexit√©
                try:
                    tree = ast.parse(content)
                    
                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            performance_stats['total_functions'] += 1
                            
                            # Analyser complexit√© fonction
                            complexity = 1  # Base complexity
                            for child in ast.walk(node):
                                if isinstance(child, (ast.If, ast.For, ast.While, ast.With)):
                                    complexity += 1
                                elif isinstance(child, ast.Try):
                                    complexity += 1
                            
                            if complexity > 10:
                                performance_stats['complex_functions'].append({
                                    'file': str(py_file),
                                    'function': node.name,
                                    'complexity': complexity
                                })
                        
                        # Analyser imports (impact startup)
                        elif isinstance(node, (ast.Import, ast.ImportFrom)):
                            if isinstance(node, ast.Import):
                                for alias in node.names:
                                    module = alias.name
                                    performance_stats['import_analysis'][module] = performance_stats['import_analysis'].get(module, 0) + 1
                            elif isinstance(node, ast.ImportFrom):
                                module = node.module
                                if module:
                                    performance_stats['import_analysis'][module] = performance_stats['import_analysis'].get(module, 0) + 1
                
                except SyntaxError:
                    pass  # Ignorer fichiers avec erreurs syntaxe
                    
            except Exception:
                pass
        
        # R√©sultats analyse
        print(f'- **Total lignes Python**: {performance_stats[\"total_lines\"]:,}')
        print(f'- **Total fonctions**: {performance_stats[\"total_functions\"]:,}')
        print(f'- **Fichiers longs** (>500 lignes): {len(performance_stats[\"long_files\"])}')
        print(f'- **Fonctions complexes** (>10): {len(performance_stats[\"complex_functions\"])}')
        print('')
        
        # Top imports (impact startup)
        if performance_stats['import_analysis']:
            print('### üì¶ Top Imports (Impact Startup)')
            sorted_imports = sorted(performance_stats['import_analysis'].items(), key=lambda x: x[1], reverse=True)
            for module, count in sorted_imports[:10]:
                print(f'- **{module}**: {count} usages')
            print('')
        
        # Fichiers probl√©matiques performance
        if performance_stats['long_files']:
            print('### üìÑ Fichiers Impact Performance')
            for filepath, lines in sorted(performance_stats['long_files'], key=lambda x: x[1], reverse=True)[:5]:
                print(f'- **{filepath}**: {lines} lignes')
            print('')
        
        # Fonctions complexes
        if performance_stats['complex_functions']:
            print('###  Fonctions Haute Complexit√©')
            sorted_complex = sorted(performance_stats['complex_functions'], key=lambda x: x['complexity'], reverse=True)
            for func_info in sorted_complex[:5]:
                print(f'- **{func_info[\"function\"]}** ({func_info[\"file\"]}): Complexit√© {func_info[\"complexity\"]}')
            print('')
        " >> performance-report.md
        
    - name: Memory Usage Profiling
      run: |
        echo "" >> performance-report.md
        echo "## Profiling M√©moire" >> performance-report.md
        echo "" >> performance-report.md
        
        # Profiling m√©moire approfondi
        python -c "
        import gc
        import sys
        import psutil
        import time
        from pathlib import Path
        
        print(' Profiling usage m√©moire...')
        
        # Test charge m√©moire
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # Simuler charge typique AGI-EVE
        test_data = []
        
        # Test 1: Chargement donn√©es cognitives
        print('Test 1: Simulation charge cognitive...')
        cognitive_data = []
        for i in range(10000):
            cognitive_data.append({
                'neuron_id': i,
                'activation': i * 0.001,
                'connections': list(range(i % 100))
            })
        
        cognitive_memory = psutil.Process().memory_info().rss / 1024 / 1024
        print(f'- **M√©moire cognitive**: {cognitive_memory - initial_memory:.1f} MB')
        
        # Test 2: Simulation univers
        print('Test 2: Simulation charge univers...')
        universe_data = []
        for i in range(5000):
            universe_data.append({
                'particle_id': i,
                'position': [i * 0.1, i * 0.2, i * 0.3],
                'velocity': [i * 0.01, i * 0.02, i * 0.03],
                'properties': {'mass': i * 0.001, 'charge': i % 2}
            })
        
        universe_memory = psutil.Process().memory_info().rss / 1024 / 1024
        print(f'- **M√©moire univers**: {universe_memory - cognitive_memory:.1f} MB')
        
        # Test 3: Charge d√©veloppement (analyse code)
        print('Test 3: Simulation outils d√©veloppement...')
        code_analysis_data = []
        
        # Simuler analyse de tous fichiers Python
        for py_file in list(Path('.').rglob('*.py'))[:1000]:  # Limiter pour test
            if '.git' not in str(py_file):
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        code_analysis_data.append({
                            'file': str(py_file),
                            'size': len(content),
                            'lines': len(content.splitlines()),
                            'tokens': content.split()
                        })
                except:
                    pass
        
        development_memory = psutil.Process().memory_info().rss / 1024 / 1024
        print(f'- **M√©moire d√©veloppement**: {development_memory - universe_memory:.1f} MB')
        
        # Nettoyage et garbage collection
        del cognitive_data, universe_data, code_analysis_data
        gc.collect()
        
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024
        print(f'- **M√©moire apr√®s cleanup**: {final_memory:.1f} MB')
        
        # Analyse r√©sultats
        total_peak_memory = development_memory - initial_memory
        cleanup_efficiency = (development_memory - final_memory) / development_memory * 100
        
        print(f'\\n **R√©sultats Profiling M√©moire**')
        print(f'- **M√©moire initiale**: {initial_memory:.1f} MB')
        print(f'- **Pic m√©moire**: {development_memory:.1f} MB')
        print(f'- **Usage maximal**: {total_peak_memory:.1f} MB')
        print(f'- **Efficacit√© cleanup**: {cleanup_efficiency:.1f}%')
        
        # Recommandations m√©moire
        print(f'\\n **Recommandations M√©moire**')
        if total_peak_memory > 500:
            print('-  **Usage m√©moire √©lev√©**: Optimisation requise')
            print('- Impl√©menter pagination pour gros datasets')
            print('- Utiliser g√©n√©rateurs au lieu de listes')
        elif total_peak_memory > 200:
            print('- üü° **Usage m√©moire mod√©r√©**: Surveillance recommand√©e')
            print('- Optimiser structures de donn√©es')
        else:
            print('-  **Usage m√©moire optimal**: Bien g√©r√©')
        
        if cleanup_efficiency < 80:
            print('-  **Cleanup inefficace**: V√©rifier fuites m√©moire')
        else:
            print('-  **Cleanup efficace**: Bonne gestion m√©moire')
        " >> performance-report.md
        
    - name: Execution Time Benchmarks
      run: |
        echo "" >> performance-report.md
        echo "## ‚è± Benchmarks Temps Ex√©cution" >> performance-report.md
        echo "" >> performance-report.md
        
        # Benchmarks temps d'ex√©cution
        python -c "
        import time
        import timeit
        import statistics
        from pathlib import Path
        import ast
        
        print('‚è± Benchmarks temps d\\'ex√©cution...')
        
        benchmark_results = {}
        
        # Benchmark 1: Import modules
        def benchmark_imports():
            start = time.time()
            modules = ['os', 'sys', 'json', 'pathlib', 'ast', 'time']
            for module in modules:
                exec(f'import {module}')
            return time.time() - start
        
        import_time = timeit.timeit(benchmark_imports, number=10) / 10
        benchmark_results['Import Modules'] = {'time': import_time, 'unit': 'seconds'}
        print(f'- **Import modules**: {import_time:.4f}s')
        
        # Benchmark 2: Lecture fichiers
        def benchmark_file_reading():
            start = time.time()
            total_size = 0
            for py_file in list(Path('.').rglob('*.py'))[:50]:  # 50 premiers fichiers
                if '.git' not in str(py_file):
                    try:
                        with open(py_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                            total_size += len(content)
                    except:
                        pass
            return time.time() - start, total_size
        
        file_read_time, total_read_size = benchmark_file_reading()
        read_speed = total_read_size / max(file_read_time, 0.001) / 1024 / 1024  # MB/s
        benchmark_results['File Reading'] = {'time': file_read_time, 'speed': read_speed, 'unit': 'MB/s'}
        print(f'- **Lecture fichiers**: {file_read_time:.3f}s ({read_speed:.1f} MB/s)')
        
        # Benchmark 3: Parsing AST
        def benchmark_ast_parsing():
            start = time.time()
            parsed_files = 0
            for py_file in list(Path('.').rglob('*.py'))[:20]:  # 20 fichiers
                if '.git' not in str(py_file):
                    try:
                        with open(py_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                            ast.parse(content)
                            parsed_files += 1
                    except:
                        pass
            return time.time() - start, parsed_files
        
        ast_time, parsed_count = benchmark_ast_parsing()
        parse_speed = parsed_count / max(ast_time, 0.001)  # fichiers/s
        benchmark_results['AST Parsing'] = {'time': ast_time, 'speed': parse_speed, 'unit': 'files/s'}
        print(f'- **Parsing AST**: {ast_time:.3f}s ({parse_speed:.1f} fichiers/s)')
        
        # Benchmark 4: Calculs intensifs (simulation)
        def benchmark_computation():
            start = time.time()
            # Simulation calculs math√©matiques
            total = 0
            for i in range(100000):
                total += i ** 2 * 0.001
            return time.time() - start
        
        compute_time = benchmark_computation()
        compute_speed = 100000 / compute_time  # op√©rations/s
        benchmark_results['Computation'] = {'time': compute_time, 'speed': compute_speed, 'unit': 'ops/s'}
        print(f'- **Calculs intensifs**: {compute_time:.3f}s ({compute_speed:.0f} ops/s)')
        
        # Analyse comparative
        print(f'\\n **Analyse Performance Comparative**')
        
        # √âtablir baseline (temps acceptable)
        performance_targets = {
            'Import Modules': 0.01,      # 10ms
            'File Reading': 1.0,         # 1s pour 50 fichiers
            'AST Parsing': 2.0,          # 2s pour 20 fichiers
            'Computation': 0.1           # 100ms pour 100k op√©rations
        }
        
        performance_score = 0
        total_benchmarks = len(benchmark_results)
        
        for benchmark, target in performance_targets.items():
            if benchmark in benchmark_results:
                actual_time = benchmark_results[benchmark]['time']
                if actual_time <= target:
                    performance_score += 1
                    print(f' **{benchmark}**: Performance excellente ({actual_time:.3f}s ‚â§ {target}s)')
                elif actual_time <= target * 2:
                    performance_score += 0.5
                    print(f'üü° **{benchmark}**: Performance acceptable ({actual_time:.3f}s ‚â§ {target*2}s)')
                else:
                    print(f'üî¥ **{benchmark}**: Performance d√©grad√©e ({actual_time:.3f}s > {target*2}s)')
        
        overall_performance = (performance_score / total_benchmarks) * 100
        print(f'\\n **Score Performance Global**: {overall_performance:.1f}%')
        
        # Recommandations optimisation
        print(f'\\n **Recommandations Optimisation**')
        if overall_performance >= 90:
            print('-  **Performance excellente**: Maintenir optimisations actuelles')
        elif overall_performance >= 70:
            print('- üü° **Performance correcte**: Optimisations cibl√©es possibles')
            print('- Profiler les op√©rations les plus lentes')
        else:
            print('- üî¥ **Performance d√©grad√©e**: Optimisation urgente requise')
            print('- Audit approfondi algorithmes')
            print('- Consid√©rer parall√©lisation')
            print('- Optimiser I/O et structures donn√©es')
        " >> performance-report.md
        
    - name: Generate Performance Optimization Recommendations
      run: |
        echo "" >> performance-report.md
        echo "## Plan Optimisation Performance" >> performance-report.md
        echo "" >> performance-report.md
        
        # G√©n√©ration recommandations optimisation
        python -c "
        from pathlib import Path
        import json
        
        print(' G√©n√©ration plan optimisation...')
        
        # Analyser structure projet pour optimisations cibl√©es
        optimization_plan = {
            'immediate_actions': [],
            'short_term_goals': [],
            'long_term_strategies': [],
            'monitoring_setup': []
        }
        
        # Actions imm√©diates bas√©es sur analyse
        optimization_plan['immediate_actions'] = [
            'Profiler les 10 fichiers les plus volumineux',
            'Optimiser imports redondants dans modules fr√©quents',
            'Impl√©menter cache pour op√©rations r√©p√©titives',
            'Ajouter monitoring m√©moire en temps r√©el'
        ]
        
        # Objectifs court terme (1-2 semaines)
        optimization_plan['short_term_goals'] = [
            'Refactoriser fonctions complexit√© >10',
            'Impl√©menter lazy loading pour gros datasets',
            'Optimiser boucles critiques avec numpy/cython',
            'Ajouter benchmarks automatis√©s CI/CD'
        ]
        
        # Strat√©gies long terme (1-3 mois)
        optimization_plan['long_term_strategies'] = [
            'Architecture microservices pour modules EVE',
            'Parall√©lisation calculs simulation',
            'Cache distribu√© pour analyse code',
            'Optimisation base donn√©es pour m√©tadonn√©es'
        ]
        
        # Configuration monitoring
        optimization_plan['monitoring_setup'] = [
            'M√©triques performance temps r√©el',
            'Alertes utilisation m√©moire >80%',
            'Tracking r√©gression performance',
            'Dashboard monitoring syst√®me complet'
        ]
        
        # Afficher plan
        for category, actions in optimization_plan.items():
            category_name = category.replace('_', ' ').title()
            print(f'### {category_name}')
            for action in actions:
                print(f'- {action}')
            print('')
        
        # Priorit√©s par module
        print('###  Priorit√©s par Module AGI-EVE')
        
        module_priorities = {
            'core/compliance': 'HAUTE - Audit constitutionnel critique',
            'eve/cognitive': 'HAUTE - Performance IA essentielle', 
            'eve/simulation': 'MOYENNE - Optimiser calculs physique',
            'eve/development': 'MOYENNE - Acc√©l√©rer analyse code',
            'eve/interfaces': 'BASSE - UI responsive suffisante',
            'tools': 'VARIABLE - Selon usage fr√©quence'
        }
        
        for module, priority in module_priorities.items():
            if Path(module).exists():
                print(f'- **{module}**: {priority}')
            else:
                print(f'- **{module}**: Non trouv√© - √Ä cr√©er si n√©cessaire')
        
        print('')
        
        # M√©triques cibles
        print('###  M√©triques Cibles Performance')
        target_metrics = {
            'Startup time': '< 5 secondes',
            'Memory usage': '< 500 MB peak',
            'File processing': '> 10 files/sec',
            'AST parsing': '> 5 files/sec',
            'CPU usage': '< 70% sustained',
            'Response time': '< 200ms interface'
        }
        
        for metric, target in target_metrics.items():
            print(f'- **{metric}**: {target}')
        
        # Sauvegarder plan JSON
        with open('performance_optimization_plan.json', 'w') as f:
            json.dump(optimization_plan, f, indent=2, ensure_ascii=False)
        
        print('\\n Plan optimisation sauvegard√©: performance_optimization_plan.json')
        " >> performance-report.md
        
    - name: Upload Performance Reports
      uses: actions/upload-artifact@v4
      with:
        name: performance-optimization-report
        path: |
          performance-report.md
          performance_optimization_plan.json
